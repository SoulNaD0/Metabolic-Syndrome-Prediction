{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metabolic Syndrome Prediction\n",
    "\n",
    "This notebook demonstrates the process of predicting metabolic syndrome using various machine learning models. The dataset contains both categorical and numerical features, and we will preprocess the data, handle missing values, and evaluate multiple models.\n",
    "\n",
    "**Dataset**: Metabolic Syndrome.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2401 entries, 0 to 2400\n",
      "Data columns (total 15 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   seqn               2401 non-null   int64  \n",
      " 1   Age                2401 non-null   int64  \n",
      " 2   Sex                2401 non-null   object \n",
      " 3   Marital            2193 non-null   object \n",
      " 4   Income             2284 non-null   float64\n",
      " 5   Race               2401 non-null   object \n",
      " 6   WaistCirc          2316 non-null   float64\n",
      " 7   BMI                2375 non-null   float64\n",
      " 8   Albuminuria        2401 non-null   int64  \n",
      " 9   UrAlbCr            2401 non-null   float64\n",
      " 10  UricAcid           2401 non-null   float64\n",
      " 11  BloodGlucose       2401 non-null   int64  \n",
      " 12  HDL                2401 non-null   int64  \n",
      " 13  Triglycerides      2401 non-null   int64  \n",
      " 14  MetabolicSyndrome  2401 non-null   int64  \n",
      "dtypes: float64(5), int64(7), object(3)\n",
      "memory usage: 281.5+ KB\n",
      "None\n",
      "\n",
      "The number of duplicated rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "raw_df = pd.read_csv(\"Metabolic Syndrome.csv\")\n",
    "\n",
    "# Display dataset information and check for duplicates\n",
    "print(raw_df.info())\n",
    "print(f\"\\nThe number of duplicated rows: {raw_df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate categorical and numerical data\n",
    "cat_data = raw_df.select_dtypes('object')\n",
    "num_data = raw_df.select_dtypes(['float64', 'int64']).iloc[:, 1:-1]\n",
    "y = raw_df['MetabolicSyndrome']\n",
    "\n",
    "# Handle missing values in categorical data using SimpleImputer\n",
    "si = SimpleImputer(strategy='most_frequent')\n",
    "cat_imp = si.fit_transform(cat_data)\n",
    "catimp = pd.DataFrame(cat_imp, columns=cat_data.columns)\n",
    "\n",
    "# Encode categorical data using LabelEncoder and OneHotEncoder\n",
    "le = LabelEncoder()\n",
    "cat_data_bin = le.fit_transform(cat_data['Sex'])\n",
    "cat_data_bin = pd.DataFrame(cat_data_bin, columns=['Sex'])\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "ohe_data = ohe.fit_transform(catimp[['Marital', 'Race']]).toarray()\n",
    "\n",
    "# Combine encoded categorical data, numerical data, and target variable\n",
    "final = np.concatenate([cat_data_bin.values, num_data.values, ohe_data], axis=1)\n",
    "final = pd.DataFrame(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "x = final.values\n",
    "y = y.values\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=444)\n",
    "\n",
    "# Scale the data using RobustScaler\n",
    "scaler = RobustScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "# Handle missing values in scaled data using KNNImputer\n",
    "kni = KNNImputer(n_neighbors=5)\n",
    "x_train_imp = kni.fit_transform(x_train_scaled)\n",
    "x_test_imp = kni.transform(x_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for KNN: {'n_neighbors': 5, 'weights': 'uniform'}\n",
      "KNN Test Accuracy: 0.817047817047817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.88      0.87       323\n",
      "           1       0.74      0.68      0.71       158\n",
      "\n",
      "    accuracy                           0.82       481\n",
      "   macro avg       0.80      0.78      0.79       481\n",
      "weighted avg       0.81      0.82      0.82       481\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# K-Nearest Neighbors (KNN) Model\n",
    "knc = KNeighborsClassifier()\n",
    "params = {\n",
    "    'n_neighbors': range(1, 30, 2),\n",
    "    'weights': ['uniform', 'distance']\n",
    "}\n",
    "gclf = GridSearchCV(knc, param_grid=params, cv=5, scoring='f1')\n",
    "gclf.fit(x_train_imp, y_train)\n",
    "print(f\"Best params for KNN: {gclf.best_params_}\")\n",
    "\n",
    "best_model = gclf.best_estimator_\n",
    "y_pred = best_model.predict(x_test_imp)\n",
    "print(f\"KNN Test Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for SVC: {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "SVC Test Accuracy: 0.817047817047817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.80      0.85       323\n",
      "           1       0.68      0.85      0.75       158\n",
      "\n",
      "    accuracy                           0.82       481\n",
      "   macro avg       0.80      0.82      0.80       481\n",
      "weighted avg       0.84      0.82      0.82       481\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Classifier (SVC) Model\n",
    "svclass = SVC(class_weight='balanced')\n",
    "params_svc = {\n",
    "    \"C\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'kernel': ['poly', 'rbf'],\n",
    "    'gamma': [0.001, 0.01, 0.1, 1]\n",
    "}\n",
    "svgrid = RandomizedSearchCV(svclass, param_distributions=params_svc, cv=5, scoring='f1', random_state=444, n_iter=5)\n",
    "svgrid.fit(x_train_imp, y_train)\n",
    "print(f\"Best params for SVC: {svgrid.best_params_}\")\n",
    "\n",
    "best_svc_model = svgrid.best_estimator_\n",
    "y_svc_pred = best_svc_model.predict(x_test_imp)\n",
    "print(f\"SVC Test Accuracy: {accuracy_score(y_test, y_svc_pred)}\")\n",
    "print(classification_report(y_test, y_svc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for Decision Tree: {'criterion': 'gini', 'max_depth': 5, 'max_leaf_nodes': 9}\n",
      "Decision Tree Test Accuracy: 0.8378378378378378\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.82      0.87       323\n",
      "           1       0.71      0.87      0.78       158\n",
      "\n",
      "    accuracy                           0.84       481\n",
      "   macro avg       0.82      0.85      0.83       481\n",
      "weighted avg       0.85      0.84      0.84       481\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Model\n",
    "dte = DecisionTreeClassifier(class_weight='balanced')\n",
    "params_dte = {\n",
    "    'max_depth': range(3, 10),\n",
    "    'max_leaf_nodes': range(3, 10),\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "tgs = GridSearchCV(dte, param_grid=params_dte, cv=5, scoring='f1')\n",
    "tgs.fit(x_train_imp, y_train)\n",
    "print(f\"Best params for Decision Tree: {tgs.best_params_}\")\n",
    "\n",
    "best_dt_model = tgs.best_estimator_\n",
    "y_pred_dt = best_dt_model.predict(x_test_imp)\n",
    "print(f\"Decision Tree Test Accuracy: {accuracy_score(y_test, y_pred_dt)}\")\n",
    "print(classification_report(y_test, y_pred_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for Logistic Regression: {'max_iter': 200, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Logistic Regression Test Accuracy: 0.8087318087318087\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.80      0.85       323\n",
      "           1       0.67      0.83      0.74       158\n",
      "\n",
      "    accuracy                           0.81       481\n",
      "   macro avg       0.79      0.81      0.79       481\n",
      "weighted avg       0.83      0.81      0.81       481\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model\n",
    "log_reg = LogisticRegression(class_weight='balanced')\n",
    "params_log_reg = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear'],\n",
    "    'max_iter': [200]\n",
    "}\n",
    "tgs_log_reg = GridSearchCV(log_reg, param_grid=params_log_reg, cv=5, scoring='f1')\n",
    "tgs_log_reg.fit(x_train_imp, y_train)\n",
    "print(f\"Best params for Logistic Regression: {tgs_log_reg.best_params_}\")\n",
    "\n",
    "best_lr_model = tgs_log_reg.best_estimator_\n",
    "y_pred_lr = best_lr_model.predict(x_test_imp)\n",
    "print(f\"Logistic Regression Test Accuracy: {accuracy_score(y_test, y_pred_lr)}\")\n",
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for Random Forest: {'criterion': 'gini', 'max_depth': 3, 'max_features': 'log2', 'max_leaf_nodes': 6, 'min_samples_leaf': 16, 'min_samples_split': 16, 'n_estimators': 200}\n",
      "Random Forest Test Accuracy: 0.8295218295218295\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.81      0.86       323\n",
      "           1       0.69      0.87      0.77       158\n",
      "\n",
      "    accuracy                           0.83       481\n",
      "   macro avg       0.81      0.84      0.82       481\n",
      "weighted avg       0.85      0.83      0.83       481\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Model\n",
    "rf = RandomForestClassifier(class_weight='balanced')\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [2, 3],\n",
    "    'max_features': ['log2', 'sqrt'],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_leaf_nodes': [4, 6],\n",
    "    'min_samples_leaf': [15, 16],\n",
    "    'min_samples_split': [15, 16]\n",
    "}\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=444)\n",
    "rfclf = GridSearchCV(rf, param_grid=param_grid_rf, cv=stratified_kfold, scoring='f1')\n",
    "rfclf.fit(x_train_imp, y_train)\n",
    "print(f\"Best params for Random Forest: {rfclf.best_params_}\")\n",
    "\n",
    "best_rf_model = rfclf.best_estimator_\n",
    "y_pred_rf = best_rf_model.predict(x_test_imp)\n",
    "print(f\"Random Forest Test Accuracy: {accuracy_score(y_test, y_pred_rf)}\")\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we preprocessed the dataset, handled missing values, and evaluated several machine learning models for predicting metabolic syndrome. The models included K-Nearest Neighbors (KNN), Support Vector Classifier (SVC), Decision Tree, Logistic Regression, and Random Forest. \n",
    "\n",
    "### Results:\n",
    "- **Best Model**: The **Decision Tree** model performed the best with an accuracy of **83.78%** and an F1-score of **0.87** for class 0 (no metabolic syndrome) and **0.78** for class 1 (metabolic syndrome).\n",
    "- **Runner-Up**: The **Random Forest** model also performed well, achieving an accuracy of **82.95%** and an F1-score of **0.86** for class 0 and **0.77** for class 1.\n",
    "\n",
    "### Model Comparison\n",
    "\n",
    "| Model               | Accuracy | F1-Score (Class 0) | F1-Score (Class 1) |\n",
    "|---------------------|----------|--------------------|--------------------|\n",
    "| KNN                 | 81.70%   | 0.87               | 0.71               |\n",
    "| SVC                 | 81.70%   | 0.85               | 0.75               |\n",
    "| Decision Tree       | 83.78%   | 0.87               | 0.78               |\n",
    "| Logistic Regression | 80.87%   | 0.85               | 0.74               |\n",
    "| Random Forest       | 82.95%   | 0.86               | 0.77               |\n",
    "\n",
    "### Insights:\n",
    "- The Decision Tree model likely performed well due to its ability to capture non-linear relationships in the data without overfitting, as indicated by the hyperparameter tuning results (`max_depth=5` and `max_leaf_nodes=9`).\n",
    "- The Random Forest model, while slightly less accurate, showed robust performance across both classes, making it a good alternative for this classification task.\n",
    "\n",
    "This notebook demonstrates the effectiveness of machine learning models in predicting metabolic syndrome, with the Decision Tree model emerging as the top performer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
